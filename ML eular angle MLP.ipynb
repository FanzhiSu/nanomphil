{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "new_euler_ep_sigh.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-22de39687625>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'beta_centre_c.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'new_euler_ep_sigh.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# only sigma value for visulisation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[1;32m    959\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    533\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    534\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: new_euler_ep_sigh.txt not found."
     ]
    }
   ],
   "source": [
    "#data visualisation\n",
    "data = np.loadtxt('beta_centre_c.txt', delimiter=' ')\n",
    "\n",
    "#data_2 = np.loadtxt('new_euler_ep_sigh.txt', delimiter=' ')\n",
    "\n",
    "#sigma=np.array(data_2[:,4])# only sigma value for visulisation\n",
    "\n",
    "# epsilon=np.array(data_2[:,3])# only epsilon value\n",
    "# data=np.array(data[:], copy=False, subok=True, ndmin=2).T\n",
    "# sigma_for_ML=np.hstack([data,data_2[:,[0,1,2,4]]])# complete matrix for ML\n",
    "\n",
    "# epsilon_for_ML=np.hstack([data,data_2[:,:4]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import genextreme as gev\n",
    "# x=np.linspace(epsilon.min(),epsilon.max())\n",
    "\n",
    "# fit = gev.fit(epsilon)\n",
    "\n",
    "# pdf = gev.pdf(x, *fit)\n",
    "\n",
    "# plt.plot(x, pdf)\n",
    "\n",
    "# plt.hist(epsilon, density=True)\n",
    "# plt.xlabel('accumulative strain')\n",
    "# plt.ylabel('density')\n",
    "# print(gev.ppf(0.25,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "from scipy.stats import genextreme as gev\n",
    "\n",
    "# mean,var=scipy.stats.distributions.norm.fit(epsilon)\n",
    "# x=np.linspace(epsilon.min(),epsilon.max())\n",
    "\n",
    "# fitted_data = scipy.stats.distributions.norm.pdf(x, mean, var)\n",
    "\n",
    "# plt.hist(epsilon,density=True)\n",
    "# fit = gev.fit(epsilon)\n",
    "\n",
    "# pdf = gev.pdf(x, *fit)\n",
    "\n",
    "# plt.plot(x, pdf,'b-',label='extreme value distribution')\n",
    "\n",
    "# plt.plot(x,fitted_data,'r-',label='normal distribution')\n",
    "# plt.xlabel('hydrostatic stress')\n",
    "# # plt.ylabel('density')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge openturns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openturns as ot\n",
    "# sample = ot.Sample([[p] for p in epsilon])\n",
    "\n",
    "# gev = ot.GeneralizedExtremeValueFactory().buildAsGeneralizedExtremeValue(sample)\n",
    "# print (gev)# mu:location, xi: shape, sigma:scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import genextreme as gev\n",
    "# x=np.linspace(epsilon.min(),epsilon.max())\n",
    "# fit=gev.pdf(x,-0.32785,0.0150714,0.00528845)\n",
    "# plt.plot(x,fit)\n",
    "# plt.hist(epsilon,density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.12 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from scipy.stats import norm\n",
    "\n",
    "mean,var=scipy.stats.distributions.norm.fit(sigma)\n",
    "boundary_1=norm.ppf(0.20, loc=mean, scale=var)\n",
    "boundary_2=norm.ppf(0.40, loc=mean, scale=var)\n",
    "# boundary_3=norm.ppf(0.60, loc=mean, scale=var)\n",
    "# boundary_4=norm.ppf(0.60, loc=mean, scale=var)\n",
    "\n",
    "\n",
    "zeros=np.zeros(1000)\n",
    "\n",
    "sigma_zero=np.column_stack([sigma_for_ML,zeros])\n",
    "for i in range(0,len(sigma_for_ML[:,-1])):\n",
    "    if sigma_zero[i,4]< boundary_1:\n",
    "        sigma_zero[i,-1]=0\n",
    "    elif boundary_1<sigma_zero[i,4]< boundary_2:\n",
    "        sigma_zero[i,-1]=1\n",
    "#     elif boundary_2<sigma_zero[i,3]< boundary_3:\n",
    "#         sigma_zero[i,-1]=2\n",
    "    else:\n",
    "        sigma_zero[i,-1]=3\n",
    "        \n",
    "sigma_label=sigma_zero[:,[0,1,2,3,5]]\n",
    "\n",
    "X_1=sigma_label[:,:4]\n",
    "y_1=sigma_label[:,-1].astype(int)\n",
    "\n",
    "# demonstrate data normalization with sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# create scaler\n",
    "scaler = MinMaxScaler()\n",
    "# fit and transform in one step\n",
    "normalized = scaler.fit_transform(X_1)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(normalized, y_1,  test_size=0.3, train_size=0.7, stratify=y_1,\n",
    "                                                     random_state=1)\n",
    "clf = MLPClassifier(random_state=1, max_iter=2000).fit(X_train, y_train)\n",
    "clf.predict_proba(X_test[:1])\n",
    "\n",
    "\n",
    "clf.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "mean,var=scipy.stats.distributions.norm.fit(sigma)\n",
    "boundary_1=norm.ppf(0.33, loc=mean, scale=var)\n",
    "boundary_2=norm.ppf(0.66, loc=mean, scale=var)\n",
    "# boundary_3=norm.ppf(0.75, loc=mean, scale=var)\n",
    "\n",
    "\n",
    "zeros=np.zeros(20000)\n",
    "\n",
    "sigma_zero=np.column_stack([sigma_for_ML,zeros])\n",
    "for i in range(0,len(sigma_for_ML[:,-1])):\n",
    "    if sigma_zero[i,3]< boundary_1:\n",
    "        sigma_zero[i,-1]=0\n",
    "    elif boundary_1<sigma_zero[i,3]< boundary_2:\n",
    "        sigma_zero[i,-1]=1\n",
    "#     elif boundary_2<sigma_zero[i,3]< boundary_3:\n",
    "#         sigma_zero[i,-1]=2\n",
    "    else:\n",
    "        sigma_zero[i,-1]=2\n",
    "        \n",
    "sigma_label=sigma_zero[:,[0,1,2,4]]\n",
    "        \n",
    "X_1=sigma_label[:,:3]\n",
    "y_1=sigma_label[:,-1].astype(int)\n",
    "\n",
    "# demonstrate data normalization with sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# create scaler\n",
    "scaler = MinMaxScaler()\n",
    "# fit and transform in one step\n",
    "normalized = scaler.fit_transform(X_1)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(normalized, y_1,  test_size=0.3, train_size=0.7, stratify=y_1,\n",
    "                                                    random_state=1)\n",
    "clf = DecisionTreeClassifier(max_depth=8)\n",
    "model1 = clf.fit(X_train,y_train)\n",
    "clf.score(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(epsilon,range(0,len(sigma)))\n",
    "plt.xlabel('accumulated plastic strain')\n",
    "plt.ylabel('data pt number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data labelling\n",
    "\n",
    "boundary_1=np.percentile(sigma_for_ML[:,-1],33)\n",
    "boundary_2=np.percentile(sigma_for_ML[:,-1],66)\n",
    "\n",
    "zeros=np.zeros(20000)\n",
    "\n",
    "sigma_zero=np.column_stack([sigma_for_ML,zeros])\n",
    "for i in range(0,len(sigma_for_ML[:,-1])):\n",
    "    if sigma_zero[i,3]< boundary_1:\n",
    "        sigma_zero[i,-1]=0\n",
    "    elif boundary_1<sigma_zero[i,3]< boundary_2:\n",
    "        sigma_zero[i,-1]=1\n",
    "    else:\n",
    "        sigma_zero[i,-1]=2\n",
    "        \n",
    "sigma_label=sigma_zero[:,[0,1,2,4]]\n",
    "print(boundary_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary_3=np.percentile(epsilon_for_ML[:,-1],33)\n",
    "boundary_4=np.percentile(epsilon_for_ML[:,-1],66)\n",
    "\n",
    "epsilon_zero=np.column_stack([epsilon_for_ML,zeros])\n",
    "\n",
    "for i in range(0,len(epsilon_for_ML[:,-1])):\n",
    "    if epsilon_zero[i,3] < boundary_3:\n",
    "        epsilon_zero[i,-1]=0\n",
    "    elif boundary_3 < epsilon_zero[i,3]< boundary_4:\n",
    "        epsilon_zero[i,-1]=1\n",
    "    else:\n",
    "        epsilon_zero[i,-1]=2\n",
    "        \n",
    "epsilon_label=epsilon_zero[:,[0,1,2,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=sigma_label[:,:3]\n",
    "y=sigma_label[:,-1].astype(int)\n",
    "sigma_zero\n",
    "sigma_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=0.3, train_size=0.7,stratify=y, random_state=1)\n",
    "clf = MLPClassifier(random_state=1, max_iter=400).fit(X_train, y_train)\n",
    "clf.predict_proba(X_test[:1])\n",
    "\n",
    "print(clf.predict(X_test))\n",
    "print(y_test)\n",
    "\n",
    "clf.score(X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1=epsilon_label[:,:3]\n",
    "y_1=epsilon_label[:,-1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_1, y_1,  test_size=0.3, train_size=0.7, stratify=y,\n",
    "                                                    random_state=1)\n",
    "clf = MLPClassifier(random_state=1, max_iter=400).fit(X_train, y_train)\n",
    "clf.predict_proba(X_test[:1])\n",
    "print('predicte',clf.predict(X_test))\n",
    "print('real',y_test)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data labelling\n",
    "\n",
    "boundary_1=np.percentile(sigma_for_ML[:,-1],25)\n",
    "boundary_2=np.percentile(sigma_for_ML[:,-1],50)\n",
    "boundary_3=np.percentile(sigma_for_ML[:,-1],75)\n",
    "\n",
    "\n",
    "\n",
    "sigma_zero=np.column_stack([sigma_for_ML,zeros])\n",
    "for i in range(0,len(sigma_for_ML[:,-1])):\n",
    "    if sigma_zero[i,3]< boundary_1:\n",
    "        sigma_zero[i,-1]=0\n",
    "    elif boundary_1<sigma_zero[i,3]< boundary_2:\n",
    "        sigma_zero[i,-1]=1\n",
    "    elif boundary_2<sigma_zero[i,3]< boundary_3:\n",
    "        sigma_zero[i,-1]=2\n",
    "    else:\n",
    "        sigma_zero[i,-1]=3\n",
    "        \n",
    "sigma_label=sigma_zero[:,[0,1,2,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1=sigma_label[:,:3]\n",
    "y_1=sigma_label[:,-1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_1, y_1,  test_size=0.3, train_size=0.7, stratify=y,\n",
    "                                                    random_state=1)\n",
    "clf = MLPClassifier(random_state=1, max_iter=2000).fit(X_train, y_train)\n",
    "clf.predict_proba(X_test[:1])\n",
    "\n",
    "clf.predict(X_test[:5, :])\n",
    "\n",
    "clf.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data labelling\n",
    "\n",
    "boundary_1=np.percentile(epsilon_for_ML[:,-1],25)\n",
    "boundary_2=np.percentile(epsilon_for_ML[:,-1],50)\n",
    "boundary_3=np.percentile(epsilon_for_ML[:,-1],75)\n",
    "\n",
    "zeros=np.zeros(20000)\n",
    "\n",
    "epsilon_zero=np.column_stack([epsilon_for_ML,zeros])\n",
    "for i in range(0,len(sigma_for_ML[:,-1])):\n",
    "    if epsilon_zero[i,3]< boundary_1:\n",
    "        epsilon_zero[i,-1]=0\n",
    "    elif boundary_1 <epsilon_zero[i,3]< boundary_2:\n",
    "        epsilon_zero[i,-1]=1\n",
    "    elif boundary_2<epsilon_zero[i,3]< boundary_3:\n",
    "        epsilon_zero[i,-1]=2\n",
    "    else:\n",
    "        epsilon_zero[i,-1]=3\n",
    "        \n",
    "epsilon_label=epsilon_zero[:,[0,1,2,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1=epsilon_label[:,:3]\n",
    "y_1=epsilon_label[:,-1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_1, y_1,  test_size=0.3, train_size=0.7, stratify=y,\n",
    "                                                    random_state=1)\n",
    "clf = MLPClassifier(random_state=1, max_iter=2000).fit(X_train, y_train)\n",
    "clf.predict_proba(X_test[:1])\n",
    "\n",
    "clf.predict(X_test[:5, :])\n",
    "\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary_1=np.percentile(sigma_for_ML[:,-1],20)\n",
    "boundary_2=np.percentile(sigma_for_ML[:,-1],40)\n",
    "boundary_3=np.percentile(sigma_for_ML[:,-1],60)\n",
    "boundary_4=np.percentile(sigma_for_ML[:,-1],80)\n",
    "\n",
    "zeros=np.zeros(20000)\n",
    "\n",
    "sigma_zero=np.column_stack([sigma_for_ML,zeros])\n",
    "for i in range(0,len(sigma_for_ML[:,-1])):\n",
    "    if sigma_zero[i,3]< boundary_1:\n",
    "        sigma_zero[i,-1]=0\n",
    "    elif boundary_1<sigma_zero[i,3]< boundary_2:\n",
    "        sigma_zero[i,-1]=1\n",
    "    elif boundary_2<sigma_zero[i,3]< boundary_3:\n",
    "        sigma_zero[i,-1]=2\n",
    "    elif boundary_3<sigma_zero[i,3]< boundary_4:\n",
    "        sigma_zero[i,-1]=3\n",
    "    else:\n",
    "        sigma_zero[i,-1]=4\n",
    "        \n",
    "sigma_label=sigma_zero[:,[0,1,2,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_2=sigma_label[:,:3]\n",
    "y_2=sigma_label[:,-1].astype(int)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_2, y_2,  test_size=0.3, train_size=0.7, stratify=y,\n",
    "                                                    random_state=1)\n",
    "\n",
    "clf = MLPClassifier(random_state=1, max_iter=2000).fit(X_train, y_train)\n",
    "clf.predict_proba(X_test[:1])\n",
    "\n",
    "clf.predict(X_test[:5, :])\n",
    "\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary_5=np.percentile(epsilon_for_ML[:,-1],20)\n",
    "boundary_6=np.percentile(epsilon_for_ML[:,-1],40)\n",
    "boundary_7=np.percentile(epsilon_for_ML[:,-1],60)\n",
    "boundary_8=np.percentile(epsilon_for_ML[:,-1],80)\n",
    "\n",
    "\n",
    "zeros=np.zeros(20000)\n",
    "epsilon_zero=np.column_stack([epsilon_for_ML,zeros])\n",
    "for i in range(0,len(epsilon_for_ML[:,-1])):\n",
    "    if epsilon_zero[i,3]< boundary_5:\n",
    "        epsilon_zero[i,-1]=0\n",
    "    elif boundary_5<epsilon_zero[i,3]< boundary_6:\n",
    "        epsilon_zero[i,-1]=1\n",
    "    elif boundary_6<epsilon_zero[i,3]< boundary_7:\n",
    "        epsilon_zero[i,-1]=2\n",
    "    elif boundary_7<epsilon_zero[i,3]< boundary_8:\n",
    "        epsilon_zero[i,-1]=3\n",
    "    else:\n",
    "        epsilon_zero[i,-1]=4\n",
    "        \n",
    "epsilon_label=epsilon_zero[:,[0,1,2,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_4=epsilon_label[:,:3]\n",
    "y_4=epsilon_label[:,-1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_4, y_4,  test_size=0.3, train_size=0.7, stratify=y,\n",
    "                                                    random_state=1)\n",
    "clf = MLPClassifier(random_state=1, max_iter=2000).fit(X_train, y_train)\n",
    "clf.predict_proba(X_test[:1])\n",
    "\n",
    "clf.predict(X_test[:5, :])\n",
    "\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #randomlisation for cross validation\n",
    "\n",
    "# sigma_list=sigma_label.tolist()\n",
    "# epsilon_list=epsilon_label.tolist()\n",
    "\n",
    "# # Convert string column to integer\n",
    "# def str_column_to_int(dataset, column):\n",
    "# \tclass_values = [row[column] for row in dataset]\n",
    "# \tunique = set(class_values)\n",
    "# \tlookup = dict()\n",
    "# \tfor i, value in enumerate(unique):\n",
    "# \t\tlookup[value] = i\n",
    "# \tfor row in dataset:\n",
    "# \t\trow[column] = lookup[row[column]]\n",
    "# \treturn lookup\n",
    "\n",
    "# dataset=sigma_list\n",
    "# # convert class column to integers\n",
    "# str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "\n",
    "\n",
    "#print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find the min and max values for each column\n",
    "# def dataset_minmax(dataset):\n",
    "# \tminmax = list()\n",
    "# \tstats = [[min(column), max(column)] for column in zip(*dataset)]\n",
    "# \treturn stats\n",
    "\n",
    "# # Rescale dataset columns to the range 0-1\n",
    "# def normalize_dataset(dataset, minmax):\n",
    "# \tfor row in dataset:\n",
    "# \t\tfor i in range(len(row)-1):\n",
    "# \t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "            \n",
    "# # Split a dataset into k folds\n",
    "# def cross_validation_split(dataset, n_folds):\n",
    "# \tdataset_split = list()\n",
    "# \tdataset_copy = list(dataset)\n",
    "# \tfold_size = int(len(dataset) / n_folds)\n",
    "# \tfor i in range(n_folds):\n",
    "# \t\tfold = list()\n",
    "# \t\twhile len(fold) < fold_size:\n",
    "# \t\t\tindex = randrange(len(dataset_copy))\n",
    "# \t\t\tfold.append(dataset_copy.pop(index))\n",
    "# \t\tdataset_split.append(fold)\n",
    "# \treturn dataset_split\n",
    "\n",
    "# # Calculate accuracy percentage\n",
    "# def accuracy_metric(actual, predicted):\n",
    "# \tcorrect = 0\n",
    "# \tfor i in range(len(actual)):\n",
    "# \t\tif actual[i] == predicted[i]:\n",
    "# \t\t\tcorrect += 1\n",
    "# \treturn correct / float(len(actual)) * 100.0\n",
    "\n",
    "# # Evaluate an algorithm using a cross validation split\n",
    "# def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "# \tfolds = cross_validation_split(dataset, n_folds)\n",
    "# \tscores = list()\n",
    "# \tfor fold in folds:\n",
    "# \t\ttrain_set = list(folds)\n",
    "# \t\ttrain_set.remove(fold)\n",
    "# \t\ttrain_set = sum(train_set, [])\n",
    "# \t\ttest_set = list()\n",
    "# \t\tfor row in fold:\n",
    "# \t\t\trow_copy = list(row)\n",
    "# \t\t\ttest_set.append(row_copy)\n",
    "# \t\t\trow_copy[-1] = None\n",
    "# \t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "# \t\tactual = [row[-1] for row in fold]\n",
    "# \t\taccuracy = accuracy_metric(actual, predicted)\n",
    "# \t\tscores.append(accuracy)\n",
    "# \treturn scores\n",
    "\n",
    "# # Calculate neuron activation for an input\n",
    "# def activate(weights, inputs):\n",
    "# \tactivation = weights[-1] # as the bias\n",
    "# \tfor i in range(len(weights)-1):\n",
    "# \t\tactivation += weights[i] * inputs[i]\n",
    "# \treturn activation\n",
    "\n",
    "# # Transfer neuron activation\n",
    "# def transfer(activation):\n",
    "# \treturn 1.0 / (1.0 + exp(-activation))\n",
    "\n",
    "# # Forward propagate input to a network output\n",
    "# def forward_propagate(network, row):\n",
    "# \tinputs = row\n",
    "# \tfor layer in network:\n",
    "# \t\tnew_inputs = []\n",
    "# \t\tfor neuron in layer:\n",
    "# \t\t\tactivation = activate(neuron['weights'], inputs)\n",
    "# \t\t\tneuron['output'] = transfer(activation)\n",
    "# \t\t\tnew_inputs.append(neuron['output'])\n",
    "# \t\tinputs = new_inputs\n",
    "# \treturn inputs\n",
    "\n",
    "# # Calculate the derivative of an neuron output\n",
    "# def transfer_derivative(output):\n",
    "# \treturn output * (1.0 - output)\n",
    "\n",
    "# # Backpropagate error and store in neurons\n",
    "# def backward_propagate_error(network, expected):\n",
    "# \tfor i in reversed(range(len(network))):\n",
    "# \t\tlayer = network[i]\n",
    "# \t\terrors = list()\n",
    "# \t\tif i != len(network)-1:\n",
    "# \t\t\tfor j in range(len(layer)):\n",
    "# \t\t\t\terror = 0.0\n",
    "# \t\t\t\tfor neuron in network[i + 1]:\n",
    "# \t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n",
    "# \t\t\t\terrors.append(error)\n",
    "# \t\telse:\n",
    "# \t\t\tfor j in range(len(layer)):\n",
    "# \t\t\t\tneuron = layer[j]\n",
    "# \t\t\t\terrors.append(expected[j] - neuron['output'])\n",
    "# \t\tfor j in range(len(layer)):\n",
    "# \t\t\tneuron = layer[j]\n",
    "# \t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    "\n",
    "# # Update network weights with error\n",
    "# def update_weights(network, row, l_rate):\n",
    "# \tfor i in range(len(network)):\n",
    "# \t\tinputs = row[:-1]\n",
    "# \t\tif i != 0:\n",
    "# \t\t\tinputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "# \t\tfor neuron in network[i]:\n",
    "# \t\t\tfor j in range(len(inputs)):\n",
    "# \t\t\t\tneuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "# \t\t\tneuron['weights'][-1] += l_rate * neuron['delta']\n",
    "\n",
    "# # Train a network for a fixed number of epochs\n",
    "# def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "# \tfor epoch in range(n_epoch):\n",
    "# \t\tfor row in train:\n",
    "# \t\t\toutputs = forward_propagate(network, row)\n",
    "# \t\t\texpected = [0 for i in range(n_outputs)]\n",
    "# \t\t\texpected[row[-1]] = 1\n",
    "# \t\t\tbackward_propagate_error(network, expected)\n",
    "# \t\t\tupdate_weights(network, row, l_rate)\n",
    "\n",
    "# # Initialize a network\n",
    "# def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "# \tnetwork = list()\n",
    "# \thidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "# \tnetwork.append(hidden_layer)\n",
    "# \toutput_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "# \tnetwork.append(output_layer)\n",
    "# \treturn network\n",
    "\n",
    "# # Make a prediction with a network\n",
    "# def predict(network, row):\n",
    "# \toutputs = forward_propagate(network, row)\n",
    "# \treturn outputs.index(max(outputs))\n",
    "\n",
    "# # Backpropagation Algorithm With Stochastic Gradient Descent\n",
    "# def back_propagation(train, test, l_rate, n_epoch, n_hidden):\n",
    "# \tn_inputs = len(train[0]) - 1\n",
    "# \tn_outputs = len(set([row[-1] for row in train]))\n",
    "# \tnetwork = initialize_network(n_inputs, n_hidden, n_outputs)\n",
    "# \ttrain_network(network, train, l_rate, n_epoch, n_outputs)\n",
    "# \tpredictions = list()\n",
    "# \tfor row in test:\n",
    "# \t\tprediction = predict(network, row)\n",
    "# \t\tpredictions.append(prediction)\n",
    "# \treturn(predictions)\n",
    "\n",
    "\n",
    "# # evaluate algorithm\n",
    "# n_folds = 3\n",
    "# l_rate = 0.2\n",
    "# n_epoch = 500\n",
    "# n_hidden = 5\n",
    "# scores = evaluate_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden)\n",
    "# print('Scores: %s' % scores)\n",
    "# print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #randomlisation for cross validation\n",
    "\n",
    "# # Convert string column to integer\n",
    "# def str_column_to_int(dataset, column):\n",
    "# \tclass_values = [row[column] for row in dataset]\n",
    "# \tunique = set(class_values)\n",
    "# \tlookup = dict()\n",
    "# \tfor i, value in enumerate(unique):\n",
    "# \t\tlookup[value] = i\n",
    "# \tfor row in dataset:\n",
    "# \t\trow[column] = lookup[row[column]]\n",
    "# \treturn lookup\n",
    "\n",
    "# dataset=epsilon_list\n",
    "# # convert class column to integers\n",
    "# str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find the min and max values for each column\n",
    "# def dataset_minmax(dataset):\n",
    "# \tminmax = list()\n",
    "# \tstats = [[min(column), max(column)] for column in zip(*dataset)]\n",
    "# \treturn stats\n",
    "\n",
    "# # Rescale dataset columns to the range 0-1\n",
    "# def normalize_dataset(dataset, minmax):\n",
    "# \tfor row in dataset:\n",
    "# \t\tfor i in range(len(row)-1):\n",
    "# \t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "            \n",
    "# # Split a dataset into k folds\n",
    "# def cross_validation_split(dataset, n_folds):\n",
    "# \tdataset_split = list()\n",
    "# \tdataset_copy = list(dataset)\n",
    "# \tfold_size = int(len(dataset) / n_folds)\n",
    "# \tfor i in range(n_folds):\n",
    "# \t\tfold = list()\n",
    "# \t\twhile len(fold) < fold_size:\n",
    "# \t\t\tindex = randrange(len(dataset_copy))\n",
    "# \t\t\tfold.append(dataset_copy.pop(index))\n",
    "# \t\tdataset_split.append(fold)\n",
    "# \treturn dataset_split\n",
    "\n",
    "# # Calculate accuracy percentage\n",
    "# def accuracy_metric(actual, predicted):\n",
    "# \tcorrect = 0\n",
    "# \tfor i in range(len(actual)):\n",
    "# \t\tif actual[i] == predicted[i]:\n",
    "# \t\t\tcorrect += 1\n",
    "# \treturn correct / float(len(actual)) * 100.0\n",
    "\n",
    "# # Evaluate an algorithm using a cross validation split\n",
    "# def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "# \tfolds = cross_validation_split(dataset, n_folds)\n",
    "# \tscores = list()\n",
    "# \tfor fold in folds:\n",
    "# \t\ttrain_set = list(folds)\n",
    "# \t\ttrain_set.remove(fold)\n",
    "# \t\ttrain_set = sum(train_set, [])\n",
    "# \t\ttest_set = list()\n",
    "# \t\tfor row in fold:\n",
    "# \t\t\trow_copy = list(row)\n",
    "# \t\t\ttest_set.append(row_copy)\n",
    "# \t\t\trow_copy[-1] = None\n",
    "# \t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "# \t\tactual = [row[-1] for row in fold]\n",
    "# \t\taccuracy = accuracy_metric(actual, predicted)\n",
    "# \t\tscores.append(accuracy)\n",
    "# \treturn scores\n",
    "\n",
    "# # Calculate neuron activation for an input\n",
    "# def activate(weights, inputs):\n",
    "# \tactivation = weights[-1]\n",
    "# \tfor i in range(len(weights)-1):\n",
    "# \t\tactivation += weights[i] * inputs[i]\n",
    "# \treturn activation\n",
    "\n",
    "# # Transfer neuron activation\n",
    "# def transfer(activation):\n",
    "# \treturn 1.0 / (1.0 + exp(-activation))\n",
    "\n",
    "# # Forward propagate input to a network output\n",
    "# def forward_propagate(network, row):\n",
    "# \tinputs = row\n",
    "# \tfor layer in network:\n",
    "# \t\tnew_inputs = []\n",
    "# \t\tfor neuron in layer:\n",
    "# \t\t\tactivation = activate(neuron['weights'], inputs)\n",
    "# \t\t\tneuron['output'] = transfer(activation)\n",
    "# \t\t\tnew_inputs.append(neuron['output'])\n",
    "# \t\tinputs = new_inputs\n",
    "# \treturn inputs\n",
    "\n",
    "# # Calculate the derivative of an neuron output\n",
    "# def transfer_derivative(output):\n",
    "# \treturn output * (1.0 - output)\n",
    "\n",
    "# # Backpropagate error and store in neurons\n",
    "# def backward_propagate_error(network, expected):\n",
    "# \tfor i in reversed(range(len(network))):\n",
    "# \t\tlayer = network[i]\n",
    "# \t\terrors = list()\n",
    "# \t\tif i != len(network)-1:\n",
    "# \t\t\tfor j in range(len(layer)):\n",
    "# \t\t\t\terror = 0.0\n",
    "# \t\t\t\tfor neuron in network[i + 1]:\n",
    "# \t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n",
    "# \t\t\t\terrors.append(error)\n",
    "# \t\telse:\n",
    "# \t\t\tfor j in range(len(layer)):\n",
    "# \t\t\t\tneuron = layer[j]\n",
    "# \t\t\t\terrors.append(expected[j] - neuron['output'])\n",
    "# \t\tfor j in range(len(layer)):\n",
    "# \t\t\tneuron = layer[j]\n",
    "# \t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    "\n",
    "# # Update network weights with error\n",
    "# def update_weights(network, row, l_rate):\n",
    "# \tfor i in range(len(network)):\n",
    "# \t\tinputs = row[:-1]\n",
    "# \t\tif i != 0:\n",
    "# \t\t\tinputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "# \t\tfor neuron in network[i]:\n",
    "# \t\t\tfor j in range(len(inputs)):\n",
    "# \t\t\t\tneuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "# \t\t\tneuron['weights'][-1] += l_rate * neuron['delta']\n",
    "\n",
    "# # Train a network for a fixed number of epochs\n",
    "# def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "# \tfor epoch in range(n_epoch):\n",
    "# \t\tfor row in train:\n",
    "# \t\t\toutputs = forward_propagate(network, row)\n",
    "# \t\t\texpected = [0 for i in range(n_outputs)]\n",
    "# \t\t\texpected[row[-1]] = 1\n",
    "# \t\t\tbackward_propagate_error(network, expected)\n",
    "# \t\t\tupdate_weights(network, row, l_rate)\n",
    "\n",
    "# # Initialize a network\n",
    "# def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "# \tnetwork = list()\n",
    "# \thidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "# \tnetwork.append(hidden_layer)\n",
    "# \toutput_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "# \tnetwork.append(output_layer)\n",
    "# \treturn network\n",
    "\n",
    "# # Make a prediction with a network\n",
    "# def predict(network, row):\n",
    "# \toutputs = forward_propagate(network, row)\n",
    "# \treturn outputs.index(max(outputs))\n",
    "\n",
    "# # Backpropagation Algorithm With Stochastic Gradient Descent\n",
    "# def back_propagation(train, test, l_rate, n_epoch, n_hidden):\n",
    "# \tn_inputs = len(train[0]) - 1\n",
    "# \tn_outputs = len(set([row[-1] for row in train]))\n",
    "# \tnetwork = initialize_network(n_inputs, n_hidden, n_outputs)\n",
    "# \ttrain_network(network, train, l_rate, n_epoch, n_outputs)\n",
    "# \tpredictions = list()\n",
    "# \tfor row in test:\n",
    "# \t\tprediction = predict(network, row)\n",
    "# \t\tpredictions.append(prediction)\n",
    "# \treturn(predictions)\n",
    "\n",
    "\n",
    "# # evaluate algorithm\n",
    "# n_folds = 3\n",
    "# l_rate = 0.05\n",
    "# n_epoch = 500\n",
    "# n_hidden = 3\n",
    "# scores = evaluate_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden)\n",
    "# print('Scores: %s' % scores)\n",
    "# print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #randomlisation for cross validation\n",
    "\n",
    "# sigma_processed=np.random.permutation(sigma_label)\n",
    "# epsilon_processed=np.random.permutation(epsilon_label)\n",
    "# sigma_list=sigma_processed.tolist()\n",
    "\n",
    "# # Convert string column to integer\n",
    "# def str_column_to_int(dataset, column):\n",
    "# \tclass_values = [row[column] for row in dataset]\n",
    "# \tunique = set(class_values)\n",
    "# \tlookup = dict()\n",
    "# \tfor i, value in enumerate(unique):\n",
    "# \t\tlookup[value] = i\n",
    "# \tfor row in dataset:\n",
    "# \t\trow[column] = lookup[row[column]]\n",
    "# \treturn lookup\n",
    "\n",
    "# dataset=sigma_list\n",
    "# # convert class column to integers\n",
    "# str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0,3):\n",
    "#     normalised=np.divide(np.subtract(sigma_ordered[:,i],np.mean(sigma_ordered[:,i])),np.std(sigma_ordered[:,i]))\n",
    "#     sigma_ordered[:,i]=normalised\n",
    "# print(sigma_ordered)\n",
    "\n",
    "# zeros=np.zeros(1000)\n",
    "\n",
    "# sigma_label=np.column_stack([sigma_ordered[:,:3],zeros])\n",
    "# sigma_label[:334,-1]=1\n",
    "# sigma_label[334:667,-1]=2\n",
    "# sigma_label[667:1001,-1]=3\n",
    "\n",
    "# epsilon_label=np.column_stack([epsilon_ordered[:,:3],zeros])\n",
    "# epsilon_label[:301,-1]=1\n",
    "# epsilon_label[301:701,-1]=2\n",
    "# epsilon_label[701:1001,-1]=3\n",
    "\n",
    "\n",
    "\n",
    "# print(sigma_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #data label & normalisation\n",
    "\n",
    "# sigma_ordered=sigma_for_ML[sigma_for_ML[:,-1].argsort()] \n",
    "# epsilon_ordered=epsilon_for_ML[epsilon_for_ML[:,-1].argsort()]\n",
    "# for i in range(0,3):\n",
    "#     normalised=np.divide(np.subtract(sigma_for_ML[:,i],np.mean(sigma_for_ML[:,i])),np.std(sigma_for_ML[:,i]))\n",
    "#     sigma_for_ML[:,i]=normalised"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
